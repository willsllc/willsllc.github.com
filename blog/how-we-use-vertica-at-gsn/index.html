<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>How we use Vertica at GSN</title>

    <link rel="stylesheet" href="../../stylesheets/styles.css">
    <link rel="stylesheet" href="../../stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
	<style>
		.highlight { background-color: #ffff99; }
		.signature { text-align: right; font-weight: italic; border-top: dashed 1px #dcdcdc; padding-top: 1em; margin-left: 15%;}
	</style>
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Portman Wills</h1>
        <p>Pages</p>


        <p class="view"><a href="https://github.com/willsllc">View My GitHub Profile</a></p>

      </header>
      <section>
        <h2>How we use Vertica at GSN</h2>

		<p>
At <a href="http://www.gsngames.com">GSN Games</a>, we’re obsessed with two things: <strong>measuring data </strong> and <strong>moving fast</strong>.  We try to take new ideas from conception to production in 2 days.  Traditional data warehousing solutions &mdash; you know, the ones that <a href="http://www.sas.com/" title="Just kidding, SAS. I love you.">take 6 months to implement</a> &mdash; would never work for us. 		
		</p>
		<p>
So here’s how we use <a href="http://www.vertica.com"><strong>Vertica and SQL</strong></a> to collect data and still move quickly.		
		</p>
		
		<h3>A very simple schema</h3>
		<p><span class="highlight">We store all of our data in a single table, named <code>events</code>.</span> The first four columns look perfectly normal:</p>
		<ul>
			<li><code>user_id</code></li>
			<li><code>event_type_id</code></li>
			<li><code>event_time</code></li>
			<li><code>game_id</code></li>
		</ul>
		<p>The next 60 columns look a little odd:</p>
		<ul>
			<li><code>attr1</code></li>
			<li><code>attr2</code></li>
			<li><code>attr3</code></li>
			<li>...</li>
			<li><code>attr60</code></li>
		</ul>
		<p>
			<blockquote>
			“Wait, that’s the kind of database that Dilbert’s pointy-haired boss would create! YOU’RE DOING IT WRONG!”</blockquote>
		</p>
		<p>Please hold all of your nerdrage until the end of this article.</p>
		<p>
			<blockquote>
			“Ok, but I’m extremely skeptical right now.”
			</blockquote>
		</p>
		<p>
		As was I when I initially saw this design. You'll come around. 
		</p>
		<p>The first 10 columns are for <code>integer</code>s, the next 10 for <code>float</code>s, the next 10 for <code>boolean</code>s, then next 10 for <code>timestamp</code>s, and the last 20 are for <code>string</code>s. 
		</p>
		<p>
		<code>event_type_id</code> is a <a href="https://www.google.com/search?q=discriminator+column">“discriminator column”</a>: it tells us what <em>kind</em> of data is in each column in that row. 
		</p>
		<p>We have lots of different events that we record, and frequently add new ones. We keep track of what data is in what column with a big Google Spreadsheet that looks like this:</p>
		<a href="column-definitions.PNG" title="click to enlarge"><img src="column-definitions.PNG" alt="A screenshot of one section of our master column definition Google Spreadsheet"/></a>
		<p>
		So for the <code>PAYMENT_RECEIVED</code> event, <code>attr1</code> stores the amount of money the user paid us, but for the <code>TOKENS_ADJUSTED</code> event, <code>attr1</code> stores the number of tokens won or lost in a game.
		</p>
		<p>Note in the above screenshot how sparse the columns are. Most cells are unused (and therefore NULL). This would have terrible performance implications in a traditional, row-oriented database. But a column-oriented has no problem with sparse columns.</p>
		<p>
		We use this Google doc to easily <span class="highlight">generate SQL views for each event type</span>. So to the casual observer, it looks like a normal RDBMS, with one table for each event, but under the hood it's one table to rule them all, forged by Sauron.
		</p>
		<p><blockquote>“What's S-A-U-R-O-N?”</blockquote></p>
		<p>Tough crowd. Moving on...</p>
		
		
		<h3>Getting data in...</h3>
		<p>
		This schema allows our game developers and platform engineers to <strong>launch new features without involving the BI team</strong>. Our app servers write to a flat <code>.csv</code> file. So if a developer comes up with a new feature and wants to begin logging metrics, this is what they do:
		</p>
		<ol>
			<li>Create a new event type id</li>
			<li>Add a new row to the Google Doc</li>
			<li>Add some code to write lines to the standard log file</li>
		</ol>
		<p>
		The log files rollover every 10MB, at which point they are uploaded to S3 for ingestion by the data warehouse.
		</p>
		<p>Every 15 minutes, the data warehouse downloads all the log files from S3, and uses the Vertica <code>copy</code> command to write them to the <code>events</code> table.
		A typical load will add 5,000,000 rows to the table, which usually takes about 45 seconds to complete.
		</p>
		<p>
		<em>Et voilà!</em> An engineer has successfully started logging new data into Vertica, and I didn't have to do anything.
		</p>
		<p><blockquote>“Wow, you’re lazy.”</blockquote></p>
		<p>Correct! Moving on...</p>
		
		
		<h3>Getting information out...</h3>
		<p>
		We're heavy users of SQL for ad-hoc analysis. Standardized reports are great, but you can't possibly anticipate everyone's questions in advance. 
		</p>
		<p>Even though the <code>events</code> table currently has <span class="highlight">close to 100 billion records</span>, query operations are still blazingly fast. Here's a recent example:</p>
		<p>A developer (let's call him <a href="http://www.gsngames.com/staff/eric/">"Shmeric"</a>) recently added a sweepstakes to our Facebook app <a href="http://apps.facebook.com/mesmogames/?fb_source=blog&ref=blog">Games by GSN</a>. Every 24 hours, we give away a ton of tokens, and users can fill out a sweepstakes card to gain entry. He created a new <code>event_type_id</code> of 137 and decided to store the timestamp of the upcoming drawing in the <code>attr31</code> column. </p>
		<p>He then asked: "do users submit more entries as the nightly drawing gets closer?". He expected that the number of entries per hour would be low when there were 24 hours until the next drawing, then steadily increase as the drawing got closer and closer. Here's the query:</p>
		<pre><code>
select 
  hr as "Hours Until Drawing",
  1.0*cumulative/first_value(cumulative) over(order by hr desc) as "Cumulative Percent",
  1.0*entered/first_value(cumulative) over(order by hr desc) as "Hourly Percent" 
from (
  select 
    datediff('hour',attr31,event_time) as hr,
    count(distinct fb_user_id) as entered,
    sum(count(distinct fb_user_id)) over (order by datediff('hour',attr31,event_time) asc) as cumulative
  from newapi.events 
  where event_type_id=137
  and datediff('hour',attr31,event_time) between -24 and -1
  and event_time between '2012-10-10' and date_trunc('day',sysdate)
  group by 1
) z
order by 1 asc
		</pre></code>
		
		
		<p><blockquote>“You know that query could be optimized if you&mdash;”</blockquote></p>
		
		<p>I'm sure it could, but it ran in 300ms, so I don't particularly care.</p>
		<pre><code>Time: First fetch (24 rows): 314.714 ms. All rows formatted: 314.791 ms</code></pre>
		<p>Here's the output using the R <code>plot</code> function:</p>
		<p><a href="rplot.PNG"><img src="rplot.PNG"/></a></p>
		<p>Interestingly, it turned out that the relationship was actually <strong>the opposite of what was predicted</strong>: the number of entries goes down as the drawing approaches (bottom left cell of the plot). Our team is currently at work tweaking the messaging to make it more clear to users who the sweepstakes works. </p>
		
		<h3>Wrapping up</h3>
		<p>So in the above example, a developer built a feature, launched it, and measured it without requiring any new tables or load scripts. </p>
		<p>Although the single table approach is unorthodox (and smells funny when you first encounter it), it has a number of advantages. </p>
		<p>To be clear: this would never work in a traditional RDBMS. (What columns would you index? All of them?) But with Vertica, a query against tens of billions of records can return in under a second. Columnar databases rule.</p> 
		
		<p class="signature">
		<strong>Portman Wills</strong> leads the BI &amp; Analytics team at GSN Digital. <br/>
		He's always ready to talk about Vertica, data science, or last night's episode of The League. Drop him a line at <strong>pwills@gsn.com</strong>, <a href="http://www.linkedin.com/in/portmanwills">connect on LinkedIn</a>, or check out all of the <a href="http://www.gsngames.com/jobs/">current positions</a> open at GSN.<br/>
		</p>
	  </section>
	  <footer>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="../../javascripts/scale.fix.js"></script>
    
  </body>
</html>